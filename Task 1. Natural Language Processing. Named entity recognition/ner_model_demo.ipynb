{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b40db488",
   "metadata": {},
   "source": [
    "#  NER demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e993fa2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertForTokenClassification, BertTokenizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# define a global variables\n",
    "TEST_SIZE = 0.2\n",
    "BATCH_SIZE = 32\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "# read dataset CSV file\n",
    "ner_df = pd.read_csv(\"Dataset.csv\")\n",
    "tokens = ner_df[\"Words\"]\n",
    "labels = ner_df[\"Labels\"]\n",
    "\n",
    "# split instances of test dataset\n",
    "_ , inference_df = train_test_split(ner_df, test_size=TEST_SIZE, random_state=101)\n",
    "_ , test_df= train_test_split(inference_df, test_size=0.5, random_state=101)\n",
    "\n",
    "# check for available devices\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Current device: {device}\\n\")\n",
    "\n",
    "# load model\n",
    "model_path = \"models/\"\n",
    "loaded_ner_model = BertForTokenClassification.from_pretrained(model_path).to(device)\n",
    "\n",
    "# create label map for encoding entities\n",
    "label_map = {\"O\": 1, \n",
    "             \"B-mountain\": 2, \"I-mountain\": 2,\n",
    "             \"B-country\": 3, \"I-country\": 3,\n",
    "             \"B-location\": 4, \"I-location\": 4,\n",
    "             \"B-scientist\": 5, \"I-scientist\": 5,\n",
    "             \"B-astronomicalobject\": 6,  \"I-astronomicalobject\": 6,\n",
    "             \"B-organisation\": 7, \"I-organisation\": 7,\n",
    "             \"B-award\": 8, \"I-award\": 8,\n",
    "             \"B-misc\": 9, \"I-misc\": 9,\n",
    "             \"B-academicjournal\": 10, \"I-academicjournal\": 10,\n",
    "             \"B-university\": 11, \"I-university\": 11,\n",
    "             \"B-person\": 12, \"I-person\": 12,\n",
    "             \"B-chemicalcompound\": 13, \"I-chemicalcompound\": 13,\n",
    "             \"B-protein\": 14, \"I-protein\": 14,\n",
    "             \"B-event\": 15, \"I-event\": 15,\n",
    "             \"B-enzyme\": 16, \"I-enzyme\": 16,\n",
    "             \"B-discipline\": 17, \"I-discipline\": 17,\n",
    "             \"B-theory\": 18, \"I-theory\": 18,\n",
    "             \"B-chemicalelement\":19, \"I-chemicalelement\":19}\n",
    "\n",
    "#  define BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d20e8c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_random_single_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Preprocesses a single sentence by tokenizing and preparing it as input tensors and attention masks.\n",
    "\n",
    "    Arguments:\n",
    "    - sentence (str): The input sentence to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "    - input_tensor (torch.Tensor): Tensor containing tokenized input IDs.\n",
    "    - attention_mask (torch.Tensor): Tensor containing the attention mask.\n",
    "    \"\"\"\n",
    "    tokenized_input = tokenizer.encode_plus(\n",
    "        sentence,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "        add_special_tokens=False)\n",
    "    input_tensor = tokenized_input[\"input_ids\"]\n",
    "    attention_mask = tokenized_input[\"attention_mask\"]\n",
    "    \n",
    "    return input_tensor, attention_mask\n",
    "\n",
    "def inference_single_sentence(model, sentence_tensors):\n",
    "    \"\"\"\n",
    "    Performs inference on a single sentence using the provided model and preprocessed sentence tensors.\n",
    "\n",
    "    Arguments:\n",
    "    - model (torch.nn.Module): The pre-trained model for inference.\n",
    "    - sentence_tensors (tuple): Tuple containing input_ids and attention_mask tensors.\n",
    "\n",
    "    Returns:\n",
    "    - predicted_labels (torch.Tensor): Predicted labels for the sentence after inference.\n",
    "    \"\"\"\n",
    "    input_ids, attention_mask = sentence_tensors\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    predicted_labels = torch.argmax(outputs.logits, dim=2)\n",
    "    mask = attention_mask.squeeze() > 0\n",
    "    predicted_labels = predicted_labels.squeeze()[mask]\n",
    "    \n",
    "    return predicted_labels\n",
    "\n",
    "def get_entities_from_predictions(sentence, predicted_labels):\n",
    "    \"\"\"\n",
    "    Extracts entities from a sentence based on predicted labels.\n",
    "\n",
    "    Arguments:\n",
    "    - sentence (str): The original sentence.\n",
    "    - predicted_labels (torch.Tensor): Predicted labels for the tokens in the sentence.\n",
    "\n",
    "    Returns:\n",
    "    - entities (list): List of tokens identified as entities based on predicted labels.\n",
    "    \"\"\"\n",
    "    tokenized_sentence = tokenizer.tokenize(sentence)\n",
    "    entity_indices = [i for i, label in enumerate(predicted_labels)]\n",
    "    entities = [tokenized_sentence[i] for i in entity_indices]\n",
    "\n",
    "    return entities\n",
    "\n",
    "def extract_class_2_words(ziped_list):\n",
    "    \"\"\"\n",
    "    Extracts words belonging to class 2 (mountains) from a list of token-class pairs.\n",
    "\n",
    "    Arguments:\n",
    "    - ziped_list (list): List of tuples containing token-class pairs.\n",
    "\n",
    "    Returns:\n",
    "    - output_list (list): List of mountain names extracted from the token-class pairs.\n",
    "    \"\"\"\n",
    "    previous_class = 999\n",
    "    mountings = []\n",
    "    mount = []\n",
    "    for sub_token, token_class in ziped_list:\n",
    "        if token_class == 2:\n",
    "            if previous_class == 2:\n",
    "                mount.append(sub_token)\n",
    "            else:\n",
    "                mount = [sub_token]  \n",
    "        elif previous_class == 2:\n",
    "            mountings.append(mount)  \n",
    "            mount = []\n",
    "        previous_class = token_class\n",
    "    if previous_class == 2:\n",
    "        mountings.append(mount)\n",
    "    output_list = [''.join([word.replace('##', '') for word in sublist]) for sublist in mountings]\n",
    "    return output_list\n",
    "\n",
    "def get_mountains_names_from_text(sentence, model):\n",
    "    \"\"\"\n",
    "    Extracts mountain names from a given sentence using a pre-trained BERT model.\n",
    "\n",
    "    Arguments:\n",
    "    - sentence (str): The input sentence.\n",
    "    - model: The pre-trained BERT model used for inference.\n",
    "\n",
    "    Returns:\n",
    "    - mountains (list): List of identified mountain names in the input sentence.\n",
    "    \"\"\"\n",
    "    sentence_tensors = preprocess_random_single_sentence(sentence)\n",
    "    predicted_labels = inference_single_sentence(model, sentence_tensors)\n",
    "    list_of_labels = predicted_labels.cpu().tolist()\n",
    "    entities = get_entities_from_predictions(sentence, predicted_labels)\n",
    "    tokens_labels_pairs = list(zip(entities, list_of_labels))\n",
    "    \n",
    "    mountains = extract_class_2_words(tokens_labels_pairs)\n",
    "    \n",
    "    return mountains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ac0cec",
   "metadata": {},
   "source": [
    "## Demo interface\n",
    "###### You can rerun the cell below to randomly select a sentence. The BERT NER model will attempt to find all mountains in this text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a89a7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: There is some information about earthquakes in Aristotle ' s Meteorology , in Naturalis Historia by Pliny the Elder , and in Strabo ' s Geographica\n",
      "\n",
      "Founded mountains: []\n"
     ]
    }
   ],
   "source": [
    "# Choose a random sentence\n",
    "sentence = random.choice(test_df[\"Words\"].tolist())\n",
    "\n",
    "# Extract mountain names\n",
    "mountains = get_mountains_names_from_text(sentence, model=loaded_ner_model)\n",
    "\n",
    "print(f\"Sentence: {sentence}\")\n",
    "print(f\"\\nFounded mountains: {mountains}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
